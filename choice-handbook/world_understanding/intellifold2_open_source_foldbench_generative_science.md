# IntelliFold 2（开源）与 FoldBench：生成式科学智能的“评分函数”开始迁移

- 首次整理：2026-02-08
- 来源：`sources/2026-02-08_intellifold2_open_source_foldbench_sota_paste.md`
- 一手锚点：  
  - IntelliFold GitHub：`https://github.com/IntelliGen-AI/IntelliFold`  
  - FoldBench（Nat Commun）：`https://www.nature.com/articles/s41467-025-67127-3`

## 01 一句话结论

IntelliFold 2 是否真的“全面领先”需要复现，但它已经释放了更重要的信号：生成式科学的竞争正在从“单点 SOTA 叙事”迁移到 **benchmark/回归集 + 部署可用性 + 审计可回链** 的综合评分函数；开源模型若能在公开基准上稳定复现，并把推理成本/可用性工程化，就可能成为行业底座之一。

## 02 机制链（深：把因果链补完整）

### 02.1 为什么 FoldBench 重要（Benchmark as Infrastructure）

- 结构预测现在不仅是蛋白单体，而是多种实体与相互作用（抗体-抗原、蛋白-配体、核酸等）
- 缺统一 benchmark 时，各家“胜利”无法比较，容易陷入 demo 与口径之争
- FoldBench 的意义：给出跨任务、低同源的统一评估口径（DockQ 阈值、样本设置、失败模式）

### 02.2 为什么“开源 + 可部署”会变得更值钱

- AlphaFold 3 的训练代码/数据不公开 → 社区只能复现/逼近 → 开源生态变成“可迭代的公共底座”
- 真正的普及障碍常在：依赖/推理成本/速度/显存/稳定性/可解释与回链，而不是单一分数
- 因此，项目方开始把 MFU/Kernel/Flash 版本/Server 作为“可用性卖点”写进主叙事

### 02.3 “超越 AF3”到底可能来自哪里（先列候选机制）

> 下面是“可能机制”，不是已证实结论；需要消融与对齐设置验证。

- 更好的表征（PairFormer 维度/latent scaling）→ 更强的复杂交互建模
- 更细粒度 tokenization（原子级）→ 柔性区域/局部接触更稳
- 采样策略与 ranking 改进（RL/PPO 或 training-free ranking）→ 成功率提升可能来自“选对样本”
- 难度感知 loss → 把训练注意力放到长尾困难区

## 03 一阶变量（准：决定它能否变成“底座”的少数变量）

1) **对齐口径后的可复现性**：同样采样/seed/评估设置下，提升是否稳定存在  
2) **时间-成本-精度曲线**：提升来自更深采样还是结构性效率（否则工业不可用）  
3) **失败病历与回归集**：是否公开失败类型、并能持续修复（而不是一次性报告）  
4) **部署门槛**：安装/推理/显存/多GPU、以及数据管线（MSA 等）是否可控  
5) **从结构到任务的闭环**：是否能在真实工作流里形成“结构→筛选→设计→验证”的可回归指标

## 04 三种情景（带概率）与 observables（可观测信号）

### S1：IntelliFold 2 成为强势开源底座之一（概率 35%）

- observables：
  - GitHub/社区出现独立复现与第三方评测（非项目方口径），且结论一致
  - 形成稳定的“版本→回归集→修复”节奏（issue 关闭与回归集扩张）
  - 部署门槛下降（pip/容器/推理脚本稳定），推理成本可预期

### S2：性能提升主要来自评估/采样口径差异，热度回落（概率 40%）

- observables：
  - 对齐采样次数/seed 后优势显著收敛或消失
  - 提升只在少数子任务成立，且对真实用户任务迁移不明显

### S3：行业继续分化：闭源仍强、开源在“可用性与工程化”取胜（概率 25%）

- observables：
  - 开源模型成为“可部署/可迭代”的默认底座，但最前沿精度仍被少数闭源/重算力玩家占据
  - 更多公司把优势写成“数据闭环/评测基建/工程效率”而不是“参数量”

## 05 48h / 7d / 30d 探针（最小成本把不确定性压下去）

### 48h：确认“开源到底开了什么”

- 核对：代码、权重、许可证、默认模型（v2-Flash/v2）、推理命令与依赖
- 读 release note：把“领先指标”逐条抄出，并标注评估设置（采样/seed/recyles）

### 7d：做一个“对齐口径”的小复现

- 选 FoldBench 的 20–50 个子集（覆盖 Ab-Ag 与 protein-ligand）
- 用相同采样策略跑 AF3（若可）/开源基线（Boltz/Chai/Protenix）与 IntelliFold v2
- 输出：成功率区间 + 置信区间 + 失败病历（3 类）

### 30d：验证“能不能进入真实工作流”

- 用 3 个真实任务（抗体筛选/配体共折叠/别构靶点）建回归集
- 指标不只看 DockQ，还看：运行成本、人工介入、可解释与回链、后续实验命中率代理指标

## 06 退出条件 + 重启条件

- **退出条件**（任一出现就降级为“观察名单”）：  
  - 对齐口径后优势不稳定或不可复现  
  - 性能提升主要来自更深采样导致成本不可控  
  - 部署复杂度长期居高（依赖/数据管线/显存）且无改善

- **重启条件**：  
  - 出现独立第三方复现与长期回归集  
  - 出现明确的成本曲线优化与稳定版本治理（release→回归→修复）

## 07 重述/合并（强制工序：把新材料嵌入旧网络）

把 IntelliFold 2 这条线索嵌入本库已有框架后，合并表述是：

- **生成式科学也进入“回归集 + 审计治理”时代**：不再只看论文分数，而看能否持续复现与修复失败病历。  
  - 对齐：`distilled/core_hypotheses.md` 的 H-0003（真实任务回归 + 权限/审计治理）
- **Benchmark 正在变成基础设施**：FoldBench 的意义类似“Benchmark as Infrastructure”，会重写赢家形态。  
  - 对齐：`meta/hypotheses_registry.md` 里关于评测基建的主线（具身侧已出现同构机制）
- **Signer Lens 在科学侧的等价物**：谁能为结果“签字”（可解释、可回链、可复盘）谁更可能被工业采用。  
  - 对齐：`frameworks/05_reasoning_toolkit.md` 的 D1（Signer Lens）

冲突点（需要探针解决）：

- 冲突：这是“结构性能力提升”，还是“采样/评估口径优势”？  
- 探针：对齐采样与评估设置做小复现，并把失败病历写成回归集。

