# MiniMax M2-her：Role-Play Bench → Online Preference Learning → Worldplay（门禁化）

> 目标：把一篇“产品/技术叙事”转成可操作的解释模型：变量、机制、门禁、探针、停机点。  
> 适用范围：角色扮演/陪伴型对话智能体（长程一致性 + 个性化 + 互动质量）。

## 0) 一句话结论

陪伴型 AI 的规模化瓶颈，会从“更大模型”迁到 **评测基建（misalignment flooring）+ 在线偏好学习去噪 + 多样性护栏**；下一代体验（Worldplay）进一步把瓶颈推到 **动态世界状态记忆** 与 **多角色协同一致性**。

## 1) 问题结构：为什么陪伴/角色扮演难

### 1.1 非可验证任务（non-verifiable）

- “有没有标准答案”决定了评测方法的上限。  
- 角色扮演里，很多回答都可能“合理”，但“明显不合理”（人设崩坏/世界观自相矛盾/越界/代替用户发言）相对可判定。

### 1.2 长程一致性是主战场

当对话拉到 50–100 turns，失败往往不是“单次答错”，而是：

- **世界观漂移**：地点/人物/关系/事实被遗忘或互相冲突
- **叙事塌缩**：节奏失控、套路化重复、角色 OOC
- **互动坍塌**：代替用户说话、自顾自、沉默、过度拒绝（把对话“关死”）

## 2) Role-Play Bench：把“评测”做成门禁（Gate）

### 2.1 关键设计：负向评估（Flooring）

- 目标不是定义“完美答案”，而是持续检测 **misalignment**（明显不对）
- 好处：可以在快速迭代中稳定做回归；坏处：容易把模型推向“保守/模板化”——因此必须配套 **多样性护栏**

### 2.2 三维度门禁（Worlds / Stories / Preferences）

把每次版本发布当作过 Gate：

- **Gate-W（Worlds）**：基础文本质量、逻辑一致性、知识/事实可靠性（长程不漂移）
- **Gate-S（Stories）**：表达多样性、叙事推进、OOC 率（不套路化、不循环）
- **Gate-P（Preferences）**：互动边界与响应性（不代替用户、不断对话钩子、不过度拒绝）

### 2.3 最小可用的“回归基建”形态（你可以直接照抄）

- **回归集**：100-turn 会话（或 5×20-turn chunk）作为硬门禁
- **失败病历字典**：把 misalignment 变成可统计标签（世界观错、角色错、空间错、越界、代说、沉默、过拒绝、重复模板……）
- **对抗与噪声测试**：输入扰动/不规范 prompt/混合语言/用户字段噪声，测试稳定性
- **版本治理**：每次修复一个失败模式，就把该 case 固化进回归集（防回退）

> 这在结构上与 `H-0017（Benchmark as Infrastructure）` 同构：只是从具身迁移到陪伴/角色扮演领域。

## 3) Online Preference Learning：在线信号如何不把模型带沟里

### 3.1 你以为的信号 vs 真实信号

隐式信号（regen、停留、继续聊、退出）天然有噪声：

- “停留更久”可能只是离屏
- “点赞”可能是习惯/礼貌
- “频繁 regen”可能是用户在找某个语气而不是否定内容

因此在线学习的核心不是 RLHF 这个词，而是 **信号工程 + 去噪假设 + 反作弊对照**。

### 3.2 门禁：多样性护栏（防坍缩）

在线优化最常见的失败不是“变差”，而是“变像”：

- 模型学到少数高回报模板，导致 **表达多样性坍缩**、故事失去生命力

所以需要“停机点”：

- 一旦重复率/模板化指标恶化，就提前终止训练或回滚策略

### 3.3 最小探针（7 天内能做的验证）

不用上大工程也能验证“在线学习是否在朝正确方向走”：

- **探针 A（去噪有效性）**：对“停留”引入离屏检测/滚动/二次交互等对照，估计“假停留”的比例是否显著下降
- **探针 B（多样性护栏）**：监控重复率/句式模板覆盖率/自相似度，验证“偏好优化”是否带来多样性坍缩
- **探针 C（回归门禁）**：每次在线更新后跑固定回归集，看 misalignment 病历是否回退

## 4) Worldplay：下一代体验把难题推到哪里

### 4.1 动态世界记忆（stateful world）

“记住事实”不够，需要能记住：

- 发生过什么（events）
- 因此改变了什么（state transitions）
- 接下来可能发生什么（affordances/foreshadowing）

工程化落点（最小可行，不追求完美）：

- **世界状态显式化**：维护一个可读/可写的 `WorldState`（人物关系、地点、关键物品、承诺、未竟事项）
- **写入策略**：只把“会影响未来分支”的信息写入（避免把噪声写进永恒记忆）
- **一致性检查**：每 N turns 做一次约束检查（人物是否同一、地点是否冲突、时间线是否断裂）
- **可回滚**：当发现世界状态写错时，能回退到最近一次稳定快照（否则长对话会累积错误）

### 4.2 多角色协同叙事（multi-agent narrative）

从 1v1 到群像，难点不是“多说几个人”，而是：

- **发言归属**：谁在说话（角色/旁白/系统）要稳定
- **状态隔离**：不同角色有不同知识边界（否则会“全知视角穿帮”）
- **后台推进**：用户不在场时发生的事件，必须能被解释为“世界在运转”，而不是无端跳转

最小门禁：

- 在同一世界里引入 2–3 个角色 + 旁白，跑 100-turn 回归，统计：
  - 角色混淆率（speaker confusion）
  - 空间/时间冲突率
  - OOC 率与循环率

## 5) 把这套东西落到“可证伪猜想”

如果这条路线成立，应该能观测到：

- **更长对话不更崩**：turn 数上升时，misalignment 病历不随之加速增长
- **个性化不是代说**：互动质量提升的同时，“AI 代替用户发言”类错误不增加
- **在线优化不坍缩多样性**：偏好指标上升时，表达多样性/叙事推进不被牺牲

如果出现以下信号，应当降级：

- 评测分数提升与真实留存/长会话完成率长期无相关（说明刷榜或口径偏了）
- 在线学习让模型明显更模板化、更保守（以稳定换生命力）
- 世界状态记忆引入后，“错误累积不可回滚”导致长会话灾难性坍塌

## 6) 关联材料（links）

- `sources/2026-01-29_minimax_m2_her_roleplay_bench_online_rlhf_worldplay.md`
- `meta/hypotheses_registry.md`：将新增一条“陪伴型评测基建/在线偏好学习”猜想，并与 H-0017 联结


