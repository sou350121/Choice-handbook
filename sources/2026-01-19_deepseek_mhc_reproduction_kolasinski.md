# Digest：DeepSeek mHC 复现（Kolasinski）——HC 的信号爆炸、Sinkhorn 约束与规模化稳定性

## 来源与类型

- **材料形态**：工程复现博客（系列，含实验记录与图表）
- **一手/二手**：二手（你粘贴的“机器之心编译”）+ 一手（作者博客原文，建议以原文为准）
- **原文链接**：
  - Blog 1：`https://taylorkolasinski.com/notes/mhc-reproduction/`
  - Blog 2：`https://taylorkolasinski.com/notes/mhc-reproduction-part2/`
  - 文中提到的 W&B：`wandb.ai/taylorkolasinski/mhc-part2`、`wandb.ai/taylorkolasinski/mhc-part2-stress`
- **可信度提示**：
  - 复现实验可重复性较强（多种子/多深度/多架构对比），但具体数值受训练配方/实现细节影响
  - “Amax 放大倍数”的定义、测量窗口、统计方式需以作者代码/日志为准
  - 结论更像“工程可行性 + 动力学/数值稳定性”证据，不等同于“最终 SOTA 架构结论”

## 主题标签

mHC｜HC｜Hyper-Connections｜残差流｜多流并行｜信号爆炸｜Amax｜doubly stochastic｜Sinkhorn-Knopp｜数值稳定性｜规模化｜梯度裁剪｜Layer0 canary

## 核心概念（可抽取资产）

### 1) 为什么残差连接能深：恒等映射是“守恒”

标准残差：$x + F(x)$。信息流是一条，保证梯度有干净路径。

作者的关键比喻：残差连接像“守恒定律”——限制了可能发生的事，但让深网络可预测/可训练。

### 2) HC：把单一残差流扩成 n 条并行流 + 可学习混合矩阵

HC 的直觉：把信息流“变宽”，并允许不同流之间混合。

三类矩阵（按文章叙述）：

- **H_res**：残差路径中各流的混合（跨层复合，最危险）
- **H_pre**：进入层之前如何组合
- **H_post**：层输出如何分配回各个流

风险：混合矩阵若不受约束，不仅能路由，还能放大信号；放大会随层数/训练步复合，导致数值不稳定。

### 3) 关键观测：Amax（放大能力）会在规模化下爆炸

作者复现与文章引用的量级（来自你提供内容的整理）：

- **10M 级**：HC 最高 Amax ~ 9.2（不同深度/种子波动较大）
- **27B（DeepSeek 论文引用）**：峰值 Amax ~ 3000（“极值/峰值”，非稳定常态）
- **1.73B（作者 Part2）**：HC 峰值 Amax 达到 **10924**（更“炸裂”）
- 压力测试（更激进学习率/更深网络）：Amax 可达更高（文中提到 14765）

重要结论：**HC 的不稳定性不是“噪声”而是“故障模式”**；在小模型可存活，在更大模型可能成为定时炸弹。

### 4) mHC：把关键混合矩阵投影到“不能放大”的流形上

DeepSeek 的核心修复（按你粘贴内容）：将混合矩阵约束为**双重随机（doubly stochastic）**：

- 条目非负
- 每行和为 1
- 每列和为 1

直觉：混合只能做加权平均/重排/融合，**不能放大**。

实现：Sinkhorn-Knopp 迭代将任意矩阵投影到双重随机集合（可微分）：

1) 从可学习的原始权重 $H$ 开始
2) $P = e^{H}$（保证正数）
3) 行归一化使每行和为 1
4) 列归一化使每列和为 1
5) 重复若干次（文中提到 ~20 次足够）

工程细节（文中提到的取舍）：

- **H_res** 做完整 Sinkhorn（因为跨层复合误差）
- **H_pre/H_post** 用 sigmoid 等有界方式（把算力花在最关键处）

### 5) 复现结论：mHC 的损失接近、但稳定性显著更好

10M 级（TinyShakespeare / GPT-2 结构）观察：

- HC 验证损失更低，但方差更大、Amax 在不同种子与深度下波动
- mHC 稳定：Amax ~ 1.0，几乎无方差

1.7B/2.5B 级（C4，bf16）观察（按你粘贴内容整理）：

- **loss 收敛几乎一致**（mHC 没“性能税”）
- Amax：HC 在 32 层/48 层都出现千倍到万倍级放大；mHC 始终 ~1.0
- 不稳定性往往从 **第 0 层**开始（直接面对 embedding，缺少“前置归一化把关”）
- 梯度裁剪可能“救了”不崩溃，但意味着模型容量在抵消内部混乱

## 机制总结（因果链 / 互动算子）

- **无约束混合矩阵** → 允许放大 → 放大跨层/跨步复合 → Amax 爆炸 → 训练动力学变成“抵消混乱” → 风险累积（定时炸弹）
- **流形约束（双重随机）** → 禁止放大 → 保留路由/混洗/融合能力 → Amax 固定 ~1.0 → 稳定可扩展
- **Layer0 canary**：输入端若尺度不匹配，HC 会倾向“补偿=放大”，因此第 0 层是早期预警点

## 可验证预测（假设/推演）

- 若用相同训练配方扩大模型规模、延长训练步数，HC 的 Amax 上界更可能继续上升，且更易出现不可控震荡/崩溃（尤其在更激进 LR、更多层数下）。
- 对 HC：把监控指标从 loss 扩展到 Amax（按层分解）能更早发现故障模式；尤其监控第 0 层混合矩阵。
- 对 mHC：只要关键矩阵被强约束为双重随机，Amax 应稳定接近 1.0，且跨种子方差极小。

## 输出去向建议

- **世界理解落点**：新增 `world_understanding/mhc_manifold_hyperconnections_residual_conservation.md`
- **可衍生清单**：一份“训练稳定性观测清单”（loss + Amax + layer0 + 梯度裁剪/归一化）

